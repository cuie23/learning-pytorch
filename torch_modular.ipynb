{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3RWA9lZlj6R5HCx0ckE1R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cuie23/learning-pytorch/blob/main/torch_modular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline of how to use a modular approach to model buildling"
      ],
      "metadata": {
        "id": "RjYnBrrG4bE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zFNGO3BIeo6J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path to data folder\n",
        "data_path = Path(\"folder\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS2H_zUAetlp",
        "outputId": "58006ce4-a906-4abb-e5fa-5ca0be4495a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find folder/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modular = Path(\"modular\")\n",
        "if not(modular.is_dir()):\n",
        "  modular.mkdir(parents=True, exist_ok=True)  # make modular directory"
      ],
      "metadata": {
        "id": "qQQMuWKDkyaF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/data_setup.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision  import datasets, transforms\n",
        "\n",
        "# Create PyTorch DataLoaders for image classification data.\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "\n",
        "  # Use ImageFolder to create datasets\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes  # list of target classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3BBJoBZeuqs",
        "outputId": "4ea4ca4b-8f31-4b20-8689-f7fd6f459edc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to build model"
      ],
      "metadata": {
        "id": "cwhhuQZJqbu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/model_builder.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# CNN\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "      super().__init__()\n",
        "      self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "      )\n",
        "      self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "      )\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "      )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      x = self.conv_block_1(x)\n",
        "      x = self.conv_block_2(x)\n",
        "      x = self.classifier(x)\n",
        "      return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RXfRW1akfPd",
        "outputId": "255535ac-7d63-4b1e-8a42-e1b6c078f161"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can directly import TinyVGG from *modular*"
      ],
      "metadata": {
        "id": "WTjlHAQPm1OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from modular import model_builder\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class_names = ['1', '2', '3']\n",
        "\n",
        "model = model_builder.TinyVGG(input_shape=3,\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "ahe4elGrmyCI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for training/testing"
      ],
      "metadata": {
        "id": "Fj-vAhm0qX_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/engine.py\n",
        "# code for training/testing\n",
        "\n",
        "import torch\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  model.train()\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    print(f\"Training batch: {batch + 1}/{len(dataloader)}\")\n",
        "\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()   # accumulate loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim = 1), dim = 1)\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  if len(dataloader) == 0:\n",
        "    return 0, 0\n",
        "\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          test_pred = model(X)\n",
        "\n",
        "          loss = loss_fn(test_pred, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          test_pred_labels = test_pred.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "        train_dataloader: torch.utils.data.DataLoader,\n",
        "        test_dataloader: torch.utils.data.DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        loss_fn: torch.nn.Module,\n",
        "        epochs: int,\n",
        "        device: torch.device) -> Dict[str, List]:\n",
        "\n",
        "  # results dict\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  for i in range(epochs):\n",
        "    train_loss, train_acc = train_step(model = model,\n",
        "                                       dataloader = train_dataloader,\n",
        "                                       loss_fn = loss_fn,\n",
        "                                       optimizer = optimizer,\n",
        "                                       device  = device)\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "                                    dataloader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    device=device)\n",
        "\n",
        "    print(\n",
        "          f\"Epoch: {i+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vah0V-79nJsE",
        "outputId": "dd279a8d-f607-41aa-8ec8-8f57dc64969a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model"
      ],
      "metadata": {
        "id": "afFQY1IcqVmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/utils.py\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "\n",
        "  # Create directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\")\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  print(f\"Saving model to \\'{model_save_path}\\'\")\n",
        "  torch.save(obj=model.state_dict(), f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K05wuqOpWP0X",
        "outputId": "f959ec61-c20a-481c-ffd7-4c9b087d5d91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do entire create + train/test + save"
      ],
      "metadata": {
        "id": "L56yITUnrblD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def walk_through_dir(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "walk_through_dir(Path(\"modular\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEMKBn1nr3--",
        "outputId": "b747175c-16da-4897-e489-d021032eab7b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 directories and 5 images in 'modular'.\n",
            "There are 0 directories and 5 images in 'modular/__pycache__'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from modular import data_setup, engine, model_builder, utils\n",
        "from torchvision import transforms\n",
        "\n",
        "# hyperparams\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Set up other input variables\n",
        "train_dir = \"folder/pizza_steak_sushi/train\"\n",
        "test_dir = \"folder/pizza_steak_sushi/test\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Training\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "print(\"hi\")\n",
        "\n",
        "# Save model\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"trained_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVQYjryyraaY",
        "outputId": "d24a198f-f2b3-4092-f828-6e8e474987f7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "H5He9uBFtW2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f3d1ae-2c77-40ab-a287-e8ed9e05caff"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 1 | train_loss: 1.1698 | train_acc: 0.3448 | test_loss: 1.0929 | test_acc: 0.1484\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 2 | train_loss: 1.1088 | train_acc: 0.3448 | test_loss: 1.0964 | test_acc: 0.4837\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 3 | train_loss: 1.0929 | train_acc: 0.3795 | test_loss: 1.0946 | test_acc: 0.2940\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 4 | train_loss: 1.0510 | train_acc: 0.4760 | test_loss: 1.0595 | test_acc: 0.5305\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 5 | train_loss: 0.9525 | train_acc: 0.6044 | test_loss: 1.0310 | test_acc: 0.5291\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 6 | train_loss: 0.9160 | train_acc: 0.5936 | test_loss: 0.9701 | test_acc: 0.5071\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 7 | train_loss: 0.8567 | train_acc: 0.6205 | test_loss: 1.0433 | test_acc: 0.3928\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 8 | train_loss: 0.7971 | train_acc: 0.6398 | test_loss: 1.1664 | test_acc: 0.4162\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 9 | train_loss: 0.7659 | train_acc: 0.6935 | test_loss: 1.2118 | test_acc: 0.4695\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 10 | train_loss: 0.7122 | train_acc: 0.7204 | test_loss: 1.1969 | test_acc: 0.4382\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 11 | train_loss: 0.6742 | train_acc: 0.7085 | test_loss: 1.4727 | test_acc: 0.4460\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 12 | train_loss: 0.6458 | train_acc: 0.7133 | test_loss: 1.4733 | test_acc: 0.4162\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 13 | train_loss: 0.7272 | train_acc: 0.7128 | test_loss: 1.6540 | test_acc: 0.3018\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 14 | train_loss: 0.6335 | train_acc: 0.7128 | test_loss: 1.4695 | test_acc: 0.4070\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 15 | train_loss: 0.7124 | train_acc: 0.6699 | test_loss: 1.3767 | test_acc: 0.4070\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 16 | train_loss: 0.5628 | train_acc: 0.7749 | test_loss: 1.5360 | test_acc: 0.3629\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 17 | train_loss: 0.6256 | train_acc: 0.7638 | test_loss: 1.3792 | test_acc: 0.4396\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 18 | train_loss: 0.5762 | train_acc: 0.7983 | test_loss: 1.2361 | test_acc: 0.4006\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 19 | train_loss: 0.5211 | train_acc: 0.8220 | test_loss: 1.7303 | test_acc: 0.3849\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 20 | train_loss: 0.4581 | train_acc: 0.7990 | test_loss: 1.8226 | test_acc: 0.4759\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 21 | train_loss: 0.4669 | train_acc: 0.8134 | test_loss: 2.7137 | test_acc: 0.3473\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 22 | train_loss: 0.3818 | train_acc: 0.8486 | test_loss: 1.9455 | test_acc: 0.4162\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 23 | train_loss: 0.3810 | train_acc: 0.8640 | test_loss: 2.1813 | test_acc: 0.4006\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 24 | train_loss: 0.3399 | train_acc: 0.8606 | test_loss: 2.2788 | test_acc: 0.4915\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 25 | train_loss: 0.2916 | train_acc: 0.8989 | test_loss: 2.7625 | test_acc: 0.4382\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 26 | train_loss: 0.2344 | train_acc: 0.9265 | test_loss: 2.6532 | test_acc: 0.4680\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 27 | train_loss: 0.2100 | train_acc: 0.9304 | test_loss: 3.1471 | test_acc: 0.4524\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 28 | train_loss: 0.1899 | train_acc: 0.9304 | test_loss: 3.2170 | test_acc: 0.4837\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 29 | train_loss: 0.2207 | train_acc: 0.9341 | test_loss: 3.8888 | test_acc: 0.4070\n",
            "Training batch: 1/4\n",
            "Training batch: 2/4\n",
            "Training batch: 3/4\n",
            "Training batch: 4/4\n",
            "Epoch: 30 | train_loss: 0.1887 | train_acc: 0.9380 | test_loss: 3.9851 | test_acc: 0.4304\n",
            "hi\n",
            "Saving model to 'models/trained_model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6jj0W2j3MyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}